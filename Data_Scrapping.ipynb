{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Data Scrapping using tweepy**"
      ],
      "metadata": {
        "id": "KSXXgl-Uj4hb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install tweepy\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TfhQQyCEgrNk",
        "outputId": "fabc0edf-68a8-482f-c901-32e865db3f2b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tweepy in /usr/local/lib/python3.11/dist-packages (4.14.0)\n",
            "Requirement already satisfied: oauthlib<4,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from tweepy) (3.2.2)\n",
            "Requirement already satisfied: requests<3,>=2.27.0 in /usr/local/lib/python3.11/dist-packages (from tweepy) (2.32.3)\n",
            "Requirement already satisfied: requests-oauthlib<2,>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from tweepy) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27.0->tweepy) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27.0->tweepy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27.0->tweepy) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27.0->tweepy) (2024.12.14)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tweepy\n",
        "\n",
        "# API credentials (replace with your own)\n",
        "API_KEY = \"XXX\"\n",
        "API_SECRET = \"XXX\"\n",
        "ACCESS_TOKEN = \"XXX\"\n",
        "ACCESS_SECRET = \"XXX\"\n",
        "BEARER_TOKEN = \"XXX\"\n",
        "\n",
        "# Authenticate with the API\n",
        "client = tweepy.Client(bearer_token=BEARER_TOKEN)\n",
        "\n",
        "# Test the connection\n",
        "try:\n",
        "    user = client.get_user(username=\"TwitterDev\")\n",
        "    print(f\"Authenticated! Username: {user.data.name}\")\n",
        "except Exception as e:\n",
        "    print(\"Error:\", e)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9LM8OIeOgtvB",
        "outputId": "44b7dca0-ca6f-48b2-8818-266e9664121e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: 'NoneType' object has no attribute 'name'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "\n",
        "# Keywords to search for\n",
        "keywords = ['program makan gratis', 'makan gratis', 'makan bergizi gratis']\n",
        "query = ' OR '.join([f'\"{keyword}\"' for keyword in keywords]) + ' lang:id'  # Combine keywords with OR operator\n",
        "\n",
        "# Search for tweets\n",
        "try:\n",
        "    tweets = client.search_recent_tweets(query=query, max_results=100)  # You can increase max_results up to 100\n",
        "\n",
        "    # Save results to CSV\n",
        "    save_path = 'scrapped_tweets.csv'\n",
        "    with open(save_path, \"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
        "        writer = csv.writer(file)\n",
        "        writer.writerow([\"Tweet ID\", \"Text\"])  # Column headers\n",
        "\n",
        "        for tweet in tweets.data:\n",
        "            writer.writerow([tweet.id, tweet.text])  # Write tweet ID and text\n",
        "\n",
        "    print(\"Tweets saved to scrapped_tweets.csv!\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(\"Error:\", e)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jL80fPdyiMsE",
        "outputId": "f0ed7a6d-710c-48df-b7e3-00068284e67d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tweets saved to scrapped_tweets.csv!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Data Cleaning**"
      ],
      "metadata": {
        "id": "5GptGii3j8-i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "tweets = pd.read_csv('scrapped_tweets.csv', delimiter=',')\n",
        "print(tweets.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QyDKI8Vdjh-8",
        "outputId": "e9edb284-f0fc-4a0d-f3f4-313e4514e925"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              Tweet ID                                               Text\n",
            "0  1880816057373675642  Mari Dukung dan Sukseskan Program Makan Bergiz...\n",
            "1  1880816034715935156  RT @NewTread_: Jadi ini video yang buat kontro...\n",
            "2  1880816031314444424  Mari Dukung dan Sukseskan Program Makan Bergiz...\n",
            "3  1880816004231839969  Mari Dukung dan Sukseskan Program Makan Bergiz...\n",
            "4  1880815973571399894  RT @anasurbaningrum: Ada anak komentar soal ra...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load custom stopwords from CSV\n",
        "stopwords_df = pd.read_csv(\"stopword.csv\")\n",
        "custom_stopwords = stopwords_df.index.tolist() # Use index to get the stopwords"
      ],
      "metadata": {
        "id": "-DJP1oiw3RSf"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import re\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt_tab')  # Download tokenizer\n",
        "\n",
        "def clean_and_remove_stopwords(text, stopwords):\n",
        "    # Remove special characters and convert to lowercase\n",
        "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", '', text)  # Remove URLs\n",
        "    text = re.sub(r\"^RT[\\s]+\", \"\", text)\n",
        "    text = re.sub(r'\\@\\w+', '', text)\n",
        "    text = re.sub(r'#(\\w+)', r'\\1', text)\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n",
        "    text = text.lower()\n",
        "\n",
        "    # Tokenize and remove stopwords\n",
        "    words = word_tokenize(text)\n",
        "    filtered_words = [word for word in words if word not in stopwords]\n",
        "    return \" \".join(filtered_words)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vqvOvL103UV_",
        "outputId": "76464d89-6951-441a-a03e-74f1dad5200c"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tweets[\"Cleaned_Tweet\"] = tweets[\"Text\"].apply(\n",
        "    lambda x: clean_and_remove_stopwords(x, custom_stopwords)\n",
        ")\n",
        "\n",
        "tweets.to_csv(\"cleaned_scrapped_tweets.csv\", index=False)"
      ],
      "metadata": {
        "id": "hYeIOAneGUAn"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cleaned_tweets = pd.read_csv('cleaned_scrapped_tweets.csv', delimiter=',')\n",
        "print(cleaned_tweets.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qlszQ-sUHSxA",
        "outputId": "61b9b6f5-7e97-47e7-8f6f-49576ebd9a67"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              Tweet ID                                               Text  \\\n",
            "0  1880816057373675642  Mari Dukung dan Sukseskan Program Makan Bergiz...   \n",
            "1  1880816034715935156  RT @NewTread_: Jadi ini video yang buat kontro...   \n",
            "2  1880816031314444424  Mari Dukung dan Sukseskan Program Makan Bergiz...   \n",
            "3  1880816004231839969  Mari Dukung dan Sukseskan Program Makan Bergiz...   \n",
            "4  1880815973571399894  RT @anasurbaningrum: Ada anak komentar soal ra...   \n",
            "\n",
            "                                       Cleaned_Tweet  \n",
            "0  mari dukung dan sukseskan program makan bergiz...  \n",
            "1  jadi ini video yang buat kontroversi makan ber...  \n",
            "2  mari dukung dan sukseskan program makan bergiz...  \n",
            "3  mari dukung dan sukseskan program makan bergiz...  \n",
            "4  ada anak komentar soal rasa menu makan gratis ...  \n"
          ]
        }
      ]
    }
  ]
}